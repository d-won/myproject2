{"cells":[{"metadata":{"_uuid":"0f481efea18c0439f5b7e12a3b492dac61e2c6da"},"cell_type":"markdown","source":"# Fruits-360 - Transfer Learning using Keras and ResNet-50\n\n* This notebook is a brief application of transfer learning on the Fruits-360 [Dataset](https://www.kaggle.com/moltean/fruits). \n* This data set consists of 42345 images of 64 fruits.\n* We compare the Transfer learning approach to the regular approach.\n* Accuracy of 98.44% is achieved within 2 epochs.\n\n\n### **Contents:**\n\n*  **1. Brief Explanation of Transfer Learning**\n*  **2. Transfer Learning using Kaggle Kernels**\n*  **3. Reading and Visualizing the Data**   \n*  **4. Building and Compiling the Models**    \n*  **5. Training and Validating the Pretrained Model** \n*  **6. Training and Validating Vanilla Model**\n*  **7. Comparison between Pretrained Models and Vanilla Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################### Library Import ###########################################\n\n# 합성곱 연산 : https://excelsior-cjh.tistory.com/180\n# 기본적인 CNN 코드에 대해서는 : https://subinium.github.io/Keras-5-1/\n\nimport os\nfrom os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nimport tensorflow as tf\n# Any results you write to the current directory are saved as output.\n\n########################################### Create a folder for Pretrained Model ###########################################\n\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n    \n!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n!cp ../input/keras-pretrained-models/resnet50* ~/.keras/models/\n\nprint(\"Available Pretrained Models:\\n\")\n!ls ~/.keras/models\n\n\n########################################### Train & Valid Set ###########################################\n\n# dimensions of our images.\nimg_width, img_height = 224, 224 \n# we set the img_width and img_height according to the pretrained models we are\n# going to use. The input size for ResNet-50 is 224 by 224 by 3.\n\ntrain_data_dir = '../input/fruits/fruits-360_dataset/fruits-360/Training/'\nvalidation_data_dir = '../input/fruits/fruits-360_dataset/fruits-360/Test/'\nnb_train_samples = 31688\nnb_validation_samples = 10657\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255, # Scaling 작업\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\n# Imagegenerator 자세한 설명은 이 주소 참고 https://tykimos.github.io/2017/03/08/CNN_Getting_Started/\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# flow_from_directory는 폴더 형식의 데이터를 불러올 때 사용\n\n# 데이터 확인\nfrom keras import layers, models\nfrom keras.preprocessing import image\nfrom glob import glob\nimport numpy as np\n\nprint('number of image files', len(image_files))\nimage_files = glob(train_data_dir + '/*/*.jp*g') # 해당 경로에 확장자를 jpg로 갖고 있는 파일 이름 모두 갖고오기\nplt.imshow(image.load_img(np.random.choice(image_files)))\n\n\n########################################### Import Pretrained Model ###########################################\n\n#import inception with pre-trained weights. do not include fully #connected layers\ninception_base = applications.ResNet50(weights='imagenet', include_top=False)\n# application은 이미 train된 것에서 top을 제외하고 network 갖고 올 수 있게 하는 함수\n\n# add a global spatial average pooling layer\nx = inception_base.output\nx = GlobalAveragePooling2D()(x)\n# global average pooling에 대해서는 여기 참고 https://poddeeplearning.readthedocs.io/ko/latest/CNN/CAM%20-%20Class%20Activation%20Map/\n\n# add a fully-connected layer\nx = Dense(512, activation='relu')(x)\n\n# and a fully connected output/classification layer\npredictions = Dense(118, activation='softmax')(x)\n\n# create the full network so we can train on it\ninception_transfer = Model(inputs=inception_base.input, outputs=predictions)\n\ninception_transfer.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### Keras 식으로 간결하게 표현해보기 \n\ninception_base = applications.ResNet50(weights='imagenet', include_top=False)\n\nmodel = models.Sequential()\nmodel.add(inception_base)\nmodel.add(layers.GlobalAveragePooling2D())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(118, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\nimport tensorflow as tf\nbatch_size = 64\nwith tf.device(\"/device:GPU:0\"):\n    history_pretrained = model.fit_generator(\n    train_generator,steps_per_epoch = int(nb_train_samples / batch_size),\n    epochs=5, shuffle = True, verbose = 1,\n    validation_data = validation_generator, validation_steps = int(nb_validation_samples / batch_size))\n    \n    \nimport matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_pretrained.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history_pretrained.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import inception with pre-trained weights. do not include fully #connected layers\ninception_base = applications.ResNet50(weights='imagenet', include_top=False)\n# application은 이미 train된 것에서 top을 제외하고 network 갖고 올 수 있게 하는 함수\n\n# add a global spatial average pooling layer\nx = inception_base.output\nx = GlobalAveragePooling2D()(x)\n# global average pooling에 대해서는 여기 참고 https://poddeeplearning.readthedocs.io/ko/latest/CNN/CAM%20-%20Class%20Activation%20Map/\n\n# add a fully-connected layer\nx = Dense(512, activation='relu')(x)\n\n# and a fully connected output/classification layer\npredictions = Dense(118, activation='softmax')(x)\n\n# create the full network so we can train on it\ninception_transfer = Model(inputs=inception_base.input, outputs=predictions)\n\ninception_transfer.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"_uuid":"de59e88cd4f4d9e378bff69f93c7aff11399fefe"},"cell_type":"code","source":"\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\nimport tensorflow as tf\nbatch_size = 64\nwith tf.device(\"/device:GPU:0\"):\n    history_pretrained = inception_transfer.fit_generator(\n    train_generator,steps_per_epoch = int(nb_train_samples / batch_size),\n    epochs=5, shuffle = True, verbose = 1,\n    validation_data = validation_generator, validation_steps = int(nb_validation_samples / batch_size))\n    \n    \nimport matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(history_pretrained.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history_pretrained.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Pretrained', 'Vanilla'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"726794a568ee1c0686fccfb56590089b141b3c96"},"cell_type":"markdown","source":"## 1. Transfer Learning\n\nIn transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.\n\nLisa Torrey and Jude Shavlik in their chapter on transfer learning describe three possible benefits to look for when using transfer learning:\n\n* Higher start. The initial skill (before refining the model) on the source model is higher than it otherwise would be.\n* Higher slope. The rate of improvement of skill during training of the source model is steeper than it otherwise would be.\n* Higher asymptote. The converged skill of the trained model is better than it otherwise would be.\n\n<center><img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Three-ways-in-which-transfer-might-improve-learning.png\"></center>\n\n\nBasically, we take a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and “fine-tune” the model with our own dataset. The idea is that this pre-trained model will either provide the initialized weights leading to a faster convergence or it will act as a fixed feature extractor for the task of interest.\n\n\n\nThese two major transfer learning scenarios look as follows:\n\n* Finetuning the convnet: Instead of random initializaion, we initialize the network with a pretrained network, like the one that has been trained on a large dataset like imagenet 1000. Rest of the training looks as usual. In this scenario the entire network needs to be retrained on the dataset of our interest\n\n* ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n\nIn this notebook we will demonstrate the first scenario.\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true},"cell_type":"markdown","source":"## 2. Transfer Learning using Kaggle Kernels\n\n### Using the Keras Pretrained Models dataset\nKaggle Kernels cannot use a network connection to download pretrained keras models. This [Dataset](https://www.kaggle.com/moltean/fruits) helps us to use our favorite pretrained models in the Kaggle Kernel environment.\n\nAll we have to do is to copy the pretrained models to the cache directory (~/.keras/models) where keras is looking for them."},{"metadata":{"_uuid":"8e127520c5fe42fc493678090ebc2effce7ea292"},"cell_type":"markdown","source":"## 3. Reading and Visualizing the Data\n### Reading the Data\n\nLike the rest of Keras, the image augmentation API is simple and powerful. We will use the **ImageDataGenerator** to fetch data and feed it to our network\n\nKeras provides the **ImageDataGenerator** class that defines the configuration for image data preparation and augmentation. Rather than performing the operations on your entire image dataset in memory, the API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training.\n\nThe data generator itself is in fact an iterator, returning batches of image samples from the directory when requested. We can configure the batch size and prepare the data generator and get batches of images by calling the **flow_from_directory()** function."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}